{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "accepted-pride",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from multiprocessing import Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "moral-discharge",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nrows=None\n",
    "# def reader(n_subarray):\n",
    "#     return pd.read_csv('../data/merged/{}/{}_band/unclean/lc_{}.csv'.format(obj,band,n_subarray), nrows=nrows, index_col = ID, dtype = {'catalogue': np.uint8, 'mag_ps': np.float32, 'magerr': np.float32, 'mjd': np.float64, ID: np.uint32})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "massive-substance",
   "metadata": {},
   "outputs": [],
   "source": [
    "nrows=None\n",
    "def reader(n_subarray):\n",
    "#     return pd.read_csv('../../data/merged/{}/{}_band/unclean/lc_{}.csv'.format(obj,band,n_subarray), nrows=nrows, comment='#', index_col = ID, dtype = {'catalogue': np.uint8, 'mag': np.float32, 'magerr': np.float32, 'mjd': np.float64, ID: np.uint32})\n",
    "    return pd.read_csv('../../data/merged/{}/{}_band/with_ssa/lc_{}.csv'.format(obj,band,n_subarray), nrows=nrows, comment='#', index_col = ID, dtype = {'catalogue': np.uint8, 'mag': np.float32, 'magerr': np.float32, 'mjd': np.float64, ID: np.uint32})\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "african-playback",
   "metadata": {},
   "outputs": [],
   "source": [
    "class save_group():\n",
    "    def __init__(self, obj='qsos', ID='uid', catalogue=0):\n",
    "        self.obj = obj\n",
    "        self.ID = ID\n",
    "        self.catalogue = catalogue\n",
    "\n",
    "    def read(self):\n",
    "        if __name__ == '__main__':\n",
    "            pool = Pool(4)\n",
    "            df_list = pool.map(reader, range(4))\n",
    "            self.df = pd.concat(df_list)#.rename(columns={'mag_ps':'mag'})\n",
    "            if catalogue != 0:\n",
    "                self.df = self.df[self.df['catalogue']==self.catalogue]\n",
    "            \n",
    "    def group_by(self, uid_subset=None):\n",
    "        if uid_subset is None:\n",
    "            uid_subset = self.df.index\n",
    "        print('uids length:',len(uid_subset))\n",
    "        print('creating uid chunks to assign to each core')\n",
    "        \n",
    "        # rather than doing this we could just split up the uids into 4 then use loc\n",
    "        # this method might be better though as it distributes load evenly across all cores\n",
    "        # wwait e could check where the splits happen and see if these line up with array_split(self.df.index.unique(),4)\n",
    "        \n",
    "        uids  = np.array_split(uid_subset,4)\n",
    "        uid_0 = uids[0].unique()\n",
    "        uid_1 = np.setdiff1d(uids[1].unique(),uid_0,assume_unique=True)\n",
    "        uid_2 = np.setdiff1d(uids[2].unique(),uid_1,assume_unique=True)\n",
    "        uid_3 = np.setdiff1d(uids[3].unique(),uid_2,assume_unique=True)\n",
    "\n",
    "        assert (len(np.unique(uid_0)) == len(uid_0)), 'uid_0 has duplicates'\n",
    "        assert (len(np.unique(uid_1)) == len(uid_1)), 'uid_1 has duplicates'\n",
    "        assert (len(np.unique(uid_2)) == len(uid_2)), 'uid_2 has duplicates'\n",
    "        assert (len(np.unique(uid_3)) == len(uid_3)), 'uid_3 has duplicates'\n",
    "\n",
    "        X = np.concatenate((uid_0,uid_1,uid_2,uid_3))\n",
    "        assert (len(np.unique(X)) == len(X)), 'There is overlap between chunks'\n",
    "\n",
    "        print('assigning chunk to each core')\n",
    "        if True:#__name__ == 'funcs.'+self.__class__.__name__:\n",
    "            pool = Pool(4)\n",
    "            df_list = pool.map(groupby_apply_single_survey, [self.df.loc[uid_0],self.df.loc[uid_1],self.df.loc[uid_2],self.df.loc[uid_3]])\n",
    "            grouped = pd.concat(df_list)\n",
    "        print('done')\n",
    "        return grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "chubby-detroit",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These nested functions aren't very pretty, but without it multiprocessing.pool isn't happy.\n",
    "def groupby_apply_single_survey(df):\n",
    "    # first define the function to apply to the groups\n",
    "    def stats(group):\n",
    "\n",
    "        # assign pandas columns to numpy arrays\n",
    "        mjds       = group['mjd'].values\n",
    "        mag        = group['mag_ps'].values\n",
    "        mag_native = group['mag'].values\n",
    "        magerr     = group['magerr'].values\n",
    "\n",
    "        # number of observations\n",
    "        n_tot   = len(group)\n",
    "\n",
    "        # time\n",
    "        mjd_min =  min(mjds)\n",
    "        mjd_max =  max(mjds)\n",
    "        mjd_ptp =  np.ptp(group['mjd'])\n",
    "\n",
    "        # magnitudes, using PS system\n",
    "        mag_min = min(mag)\n",
    "        mag_max = max(mag)\n",
    "        mag_med = -2.5*np.log10(np.median(10**(-(mag-8.9)/2.5))) + 8.9\n",
    "        mag_mean= -2.5*np.log10(np.mean  (10**(-(mag-8.9)/2.5))) + 8.9\n",
    "        mag_std = np.std(mag)\n",
    "        \n",
    "        # native (untransformed) magnitudes\n",
    "        mag_med_native  = -2.5*np.log10(np.median(10**(-(mag_native-8.9)/2.5))) + 8.9\n",
    "        mag_mean_native = -2.5*np.log10(np.mean  (10**(-(mag_native-8.9)/2.5))) + 8.9\n",
    "        \n",
    "        # magnitude errors\n",
    "        magerr_max = max(magerr)\n",
    "        magerr_med = np.median(magerr)\n",
    "        magerr_mean= np.mean(magerr)\n",
    "        \n",
    "        # using flux flux\n",
    "        flux = 10**(-(mag-8.9)/2.5)\n",
    "        fluxerr = 0.921*flux*magerr # ln(10)/2.5 ~ 0.921\n",
    "        fluxerr_mean_opt = ( flux*(fluxerr**-2) ).sum()/(fluxerr**-2).sum()\n",
    "        # calculate the optimal flux average then convert back to mags\n",
    "        mag_opt_mean_flux = -2.5*np.log10(fluxerr_mean_opt) + 8.9\n",
    "        # magerr_opt_std_flux = not clear how to calculate this. magerr_opt_std should suffice.\n",
    "\n",
    "        # optimal (inverse-variance weighted) averages (see aaa04)\n",
    "        mag_opt_mean   = ( mag*(magerr**-2) ).sum()/(magerr**-2).sum()\n",
    "        magerr_opt_std = (magerr**-2).sum()**-0.5\n",
    "\n",
    "        return {'n_tot':n_tot, 'mjd_min':mjd_min, 'mjd_max':mjd_max, 'mjd_ptp':mjd_ptp,\n",
    "                'mag_min':mag_min, 'mag_max':mag_max, 'mag_mean':mag_mean, 'mag_med':mag_med, 'mag_mean_native':mag_mean_native, 'mag_med_native':mag_med_native, 'mag_opt_mean':mag_opt_mean, 'mag_opt_mean_flux':mag_opt_mean_flux, 'mag_std':mag_std,\n",
    "                'magerr_max':magerr_max, 'magerr_mean':magerr_mean, 'magerr_med':magerr_med, 'magerr_opt_std':magerr_opt_std}\n",
    "    \n",
    "    return df.groupby(df.index.names).apply(stats).apply(pd.Series).astype({'n_tot':'uint16'})\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "sublime-description",
   "metadata": {},
   "source": [
    "# These nested functions aren't very pretty, but without it multiprocessing.pool isn't happy.\n",
    "def groupby_apply(df):\n",
    "    # first define the function to apply to the groups\n",
    "    def stats(group):\n",
    "\n",
    "        # assign pandas columns to numpy arrays\n",
    "        mjds    = group['mjd'].values\n",
    "        mag     = group['mag'].values\n",
    "        magerr  = group['magerr'].values\n",
    "\n",
    "        # number of observations\n",
    "        n_sss_r1= (group['catalogue']==1).sum()\n",
    "        n_sss_r2= (group['catalogue']==1).sum()\n",
    "        n_sdss  = (group['catalogue']==5).sum()\n",
    "        n_ps    = (group['catalogue']==7).sum()\n",
    "        n_ztf   = (group['catalogue']==11).sum()\n",
    "        n_tot   = len(group)\n",
    "\n",
    "        # time\n",
    "        mjd_min =  min(mjds)\n",
    "        mjd_max =  max(mjds)\n",
    "        mjd_ptp =  np.ptp(group['mjd'])\n",
    "\n",
    "        # magnitudes\n",
    "        mag_min = min(mag)\n",
    "        mag_max = max(mag)\n",
    "        mag_med = -2.5*np.log10(np.median(10**(-(mag-8.9)/2.5))) + 8.9\n",
    "        mag_mean= -2.5*np.log10(np.mean  (10**(-(mag-8.9)/2.5))) + 8.9\n",
    "        mag_std = np.std(mag)\n",
    "\n",
    "        # magnitude errors\n",
    "        magerr_max = max(magerr)\n",
    "        magerr_med = np.median(magerr)\n",
    "        magerr_mean= np.mean(magerr)\n",
    "        \n",
    "        # flux\n",
    "        flux = 10**(-(mag-8.9)/2.5)\n",
    "        fluxerr = 0.921*flux*magerr # ln(10)/2.5 ~ 0.921\n",
    "        fluxerr_mean_opt = ( flux*(fluxerr**-2) ).sum()/(fluxerr**-2).sum()\n",
    "        # calculate the optimal flux average then convert back to mags\n",
    "        mag_opt_mean_flux = -2.5*np.log10(fluxerr_mean_opt) + 8.9\n",
    "        # magerr_opt_std_flux = not clear how to calculate this. magerr_opt_std should suffice.\n",
    "\n",
    "        # optimal (inverse-variance weighted) averages (see aaa04)\n",
    "        mag_opt_mean   = ( mag*(magerr**-2) ).sum()/(magerr**-2).sum()\n",
    "        magerr_opt_std = (magerr**-2).sum()**-0.5\n",
    "\n",
    "        return {'n_tot':n_tot, 'n_sss_r1':n_sss_r1, 'n_sss_r2':n_sss_r2, 'n_sdss':n_sdss, 'n_ps':n_ps, 'n_ztf':n_ztf,\n",
    "                'mjd_min':mjd_min, 'mjd_max':mjd_max, 'mjd_ptp':mjd_ptp,\n",
    "                'mag_min':mag_min, 'mag_max':mag_max, 'mag_mean':mag_mean, 'mag_med':mag_med, 'mag_opt_mean':mag_opt_mean, 'mag_opt_mean_flux':mag_opt_mean_flux, 'mag_std':mag_std,\n",
    "                'magerr_max':magerr_max, 'magerr_mean':magerr_mean, 'magerr_med':magerr_med, 'magerr_opt_std':magerr_opt_std}\n",
    "    \n",
    "    return df.groupby(df.index.names).apply(stats).apply(pd.Series).astype({col:'uint16' for col in ['n_tot','n_sss_r1','n_sss_r2','n_sdss','n_ps','n_ztf']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "interstate-evolution",
   "metadata": {},
   "outputs": [],
   "source": [
    "# obj = 'calibStars'\n",
    "# ID  = 'uid_s'\n",
    "# band = 'g'\n",
    "obj = 'qsos'\n",
    "ID  = 'uid'\n",
    "band = 'r'\n",
    "survey = 'sss_r1'\n",
    "\n",
    "catalogue_dict = {'all':0, 'sss_r1':1, 'sss_r2':3, 'sdss':5, 'ps':7, 'ztf':11}\n",
    "catalogue = catalogue_dict[survey] # Only use data from named survey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "clear-liberty",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "beautiful-communist",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/disk1/hrb/anaconda3/envs/astroconda/lib/python3.6/site-packages/numpy/lib/arraysetops.py:580: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n",
      "/disk1/hrb/anaconda3/envs/astroconda/lib/python3.6/site-packages/numpy/lib/arraysetops.py:580: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n",
      "/disk1/hrb/anaconda3/envs/astroconda/lib/python3.6/site-packages/numpy/lib/arraysetops.py:580: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n",
      "/disk1/hrb/anaconda3/envs/astroconda/lib/python3.6/site-packages/numpy/lib/arraysetops.py:580: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uids length: 27202\n",
      "creating uid chunks to assign to each core\n",
      "assigning chunk to each core\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'mag_ps'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRemoteTraceback\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;31mRemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/disk1/hrb/anaconda3/envs/astroconda/lib/python3.6/site-packages/pandas/core/indexes/base.py\", line 2898, in get_loc\n    return self._engine.get_loc(casted_key)\n  File \"pandas/_libs/index.pyx\", line 70, in pandas._libs.index.IndexEngine.get_loc\n  File \"pandas/_libs/index.pyx\", line 101, in pandas._libs.index.IndexEngine.get_loc\n  File \"pandas/_libs/hashtable_class_helper.pxi\", line 1675, in pandas._libs.hashtable.PyObjectHashTable.get_item\n  File \"pandas/_libs/hashtable_class_helper.pxi\", line 1683, in pandas._libs.hashtable.PyObjectHashTable.get_item\nKeyError: 'mag_ps'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/disk1/hrb/anaconda3/envs/astroconda/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n    result = (True, func(*args, **kwds))\n  File \"/disk1/hrb/anaconda3/envs/astroconda/lib/python3.6/multiprocessing/pool.py\", line 44, in mapstar\n    return list(map(*args))\n  File \"<ipython-input-12-25ce7520ff6d>\", line 52, in groupby_apply_single_survey\n    return df.groupby(df.index.names).apply(stats).apply(pd.Series).astype({'n_tot':'uint16'})\n  File \"/disk1/hrb/anaconda3/envs/astroconda/lib/python3.6/site-packages/pandas/core/groupby/groupby.py\", line 859, in apply\n    result = self._python_apply_general(f, self._selected_obj)\n  File \"/disk1/hrb/anaconda3/envs/astroconda/lib/python3.6/site-packages/pandas/core/groupby/groupby.py\", line 892, in _python_apply_general\n    keys, values, mutated = self.grouper.apply(f, data, self.axis)\n  File \"/disk1/hrb/anaconda3/envs/astroconda/lib/python3.6/site-packages/pandas/core/groupby/ops.py\", line 220, in apply\n    res = f(group)\n  File \"<ipython-input-12-25ce7520ff6d>\", line 8, in stats\n    mag        = group['mag_ps'].values\n  File \"/disk1/hrb/anaconda3/envs/astroconda/lib/python3.6/site-packages/pandas/core/frame.py\", line 2906, in __getitem__\n    indexer = self.columns.get_loc(key)\n  File \"/disk1/hrb/anaconda3/envs/astroconda/lib/python3.6/site-packages/pandas/core/indexes/base.py\", line 2900, in get_loc\n    raise KeyError(key) from err\nKeyError: 'mag_ps'\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-da6d22435515>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mdr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mgrouped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup_by\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'elapsed: {:.3f}s'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-af4ee4616674>\u001b[0m in \u001b[0;36mgroup_by\u001b[0;34m(self, uid_subset)\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m#__name__ == 'funcs.'+self.__class__.__name__:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0mpool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m             \u001b[0mdf_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroupby_apply_single_survey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0muid_0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0muid_1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0muid_2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0muid_3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m             \u001b[0mgrouped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'done'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/disk1/hrb/anaconda3/envs/astroconda/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, func, iterable, chunksize)\u001b[0m\n\u001b[1;32m    264\u001b[0m         \u001b[0;32min\u001b[0m \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mthat\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mreturned\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m         '''\n\u001b[0;32m--> 266\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_map_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapstar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstarmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/disk1/hrb/anaconda3/envs/astroconda/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    642\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    643\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 644\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    645\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'mag_ps'"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "# nrowses = np.array([10e4, 20e4, 30e4, 40e4, 60e4, 100e4], dtype='int')\n",
    "# times = []\n",
    "\n",
    "# for nrows in nrowses:\n",
    "for band in 'r':\n",
    "    dr = save_group(obj, ID, catalogue=catalogue)\n",
    "    dr.read()\n",
    "    start = time()\n",
    "    grouped = dr.group_by() \n",
    "    end = time()\n",
    "    print('elapsed: {:.3f}s'.format(end-start))\n",
    "    grouped.to_csv('../../data/merged/{}/{}_band/grouped_stats_{}_{}.csv'.format(obj,band,band,survey))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "solar-welcome",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fifth-definition",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
